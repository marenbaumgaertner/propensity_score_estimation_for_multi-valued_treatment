---
title: "Fine tuning hyperparameter"
author: "Maren Baumg√§rtner"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

```{r, message=FALSE, echo=FALSE,warning=FALSE}
source("packages.R")
```

```{r}
source("ml_wrapper.R")
source("eval_functions.R")

set.seed(42)
options(scipen= 999)
```

# Multi-valued

Define inherently multi-valued classifieres

```{r}
classifiers <- c(
  "knn", "knn_radius", "lda", "logit",
  "mlpc", "multinom", "nb_bernulli", "nb_gaussian",
  "probability_forest", "qda", "ranger", "xgboost"
)

classifiers_to_tune <- classifiers[!(classifiers %in% c("lda", "qda"))]
```

## Create multi-valued train data

```{}
n=2000
K=3
p=15

W <- sample(seq(1, K, by = 1), replace = TRUE, n) # random W
X = matrix(runif(n*p,-pi,pi),ncol=p)
#X = matrix(sample(-0:2, n * p, replace = TRUE), nrow = n, ncol = p)

# Define a function to assign probabilities based on the sign of X[1]
assign_probabilities <- function(x) {
  p_class_1 <- ifelse(x[1] < 0, 0.7, 0.15)
  p_class_3 <- ifelse(x[1] > 0, 0.7, 0.15)
  p_class_2 <- 1 - p_class_1 - p_class_3
  probabilities <- c(p_class_1, p_class_2, p_class_3)
  return(probabilities)
}

# Apply the function to each row of X to get probabilities for W
probabilities <- t(apply(X, 1, assign_probabilities))

# Randomly sample W based on the probabilities
W <- apply(probabilities, 1, function(p) sample(1:K, size = 1, prob = p))

W.levels <- sort(unique(W))
K <- length(W.levels)

dataset <- data.frame(W=W,X)

split_indices <- sample.split(dataset$W, SplitRatio = 0.8)
training_data <- dataset[split_indices, ]
test_data <- dataset[!split_indices, ]

W  <- as.matrix(data.frame(W = training_data$W))
X  <- as.matrix(data.frame(X[split_indices, ]))
W_test <-     as.matrix(data.frame(W = test_data$W))
X_test <-     as.matrix(data.frame(X[!split_indices, ]))
```

## Use Imbens (2016) data to fine tune classifiers

```{r}
#dataname <- "Imbens_2016"
#dataname <- "Linden_2015"
dataname <- "Archarky_2023"
datapath <- paste0("data/real_data_", dataname,".R")
#datapath <- paste0("data/simulate_data_", dataname,".R")
source(datapath)
X <- as.matrix(X)
```

## Define methods

For each classifier I create a list which contains information on the classifier:

1.  Method Type:\
    Define for which type of problem the method is suitable. In my case it is all classification, right now this information is not used by the `finetuning_soft_classifier()` function, but as most of these methods are also able to do regession tasks it may be useful later on.

2.  Library Selection:

    Define the package the methods is using such that the correct package can be loaded by the `finetuning_soft_classifier()` function

3.  Parameters:

    The parameters argument contains a table of all used tuning parameters for the model. It contains information on the name of the input argument of the function, whether it is numeric or a character, and a definition of what the tuning parameter is doing.

4.  Fitting and Prediction Functions:

    Next, I define the corresponding `_fit()` and `predict.()` function.

5.  Tuning Grid:

    Lastly, I define the tuning grid for tuning parameter optimization. This grid allows the algorithm to find the optimal combination of parameters that results in the best model performance.

### 1. Multinomial logistic regression (glmnet)

-   alpha: Mixing Percentage

-   lambda: Regularization

```{r}
# define new method
multinomial_logit <- list(type = "Classification",
              library = "glmnet",
              multiclass = TRUE) 
# define parameters
multinomial_logit$parameters <- data.frame(parameter = c("alpha", "lambda"),
                  class = rep("numeric", 2),
                  label = c("Mixing Percentage", "Regularization"))

multinomial_logit$fit <- logit_fit
multinomial_logit$predict <- predict.logit_fit
# design the parameter tuning grid
multinomial_logit$grid <- expand.grid(alpha = seq(0,1,0.05))#,
                    #lambda = seq(0.0001,0.01,0.005)) # lambda tunes itself
```

### 2. Multinomial logistic regression (nnet)

multinom calls nnet() with zero hidden layers (size=0)

-   alpha: Mixing Percentage

-   lambda: Regularization

```{r}
# define new method
multinomial_logit_nnet <- list(type = "Classification",
              library = "nnet",
              multiclass = TRUE) 
# define parameters
multinomial_logit_nnet$parameters <- data.frame(parameter = c("decay"),
                  class = c("numeric"),
                  label = c("weight decay"))

multinomial_logit_nnet$fit <- multinom_fit
multinomial_logit_nnet$predict <- predict.multinom_fit
# design the parameter tuning grid
multinomial_logit_nnet$grid <- expand.grid(decay = seq(0,1,0.05))

```

### 3. kNN

-   kmax: number of neighbors considered

-   distance: parameter of Minkowski distance

```{r}
knn <- list(type = "Classification",
            library = "kknn",
            multiclass = TRUE)
params <- data.frame(parameter = c("kmax", "distance", "kernel"),
                     class = c("numeric", "numeric", ""),
                     label = c("number of neighbors considered", 
                               "parameter of Minkowski distance",
                               "Kernel to use"))
knn$params <- params
knn$fit <- knn_fit
knn$predict <- predict.knn_fit
knn$grid <- expand.grid(kmax = seq(5,20,1),
                        #distance = seq(1,5,1),
                        kernel = c("rectangular", "gaussian", "optimal"))
```

### 4. kNN (Radius)

```{r}
knn_radius <- list(type = "Classification",
            library = "kknn",
            multiclass = TRUE)
params <- data.frame(parameter = c("distance", "kernel"),
                     class = c("numeric", ""),
                     label = c("parameter of Minkowski distance",
                               "Kernel to use"))
knn_radius$params <- params
knn_radius$fit <- knn_fit
knn_radius$predict <- predict.knn_fit
knn_radius_grid <- expand.grid(distance = seq(1,5,1),
                        kernel = c("rectangular", "gaussian", "optimal"))
knn_radius$grid <- knn_radius_grid
```

### 5. Naive Bayes (Bernulli)

-   laplace:

```{r}
nb_bernulli <- list(
  type = "Classification",
  library = "naivebayes",
  multiclass = TRUE,
  params = data.frame(parameter = c("laplace"), 
                      class = c("numeric"), 
                      label = c("value of Laplace smoothing")),
  fit = nb_bernulli_fit,
  predict = predict.nb_bernulli_fit,
  grid = expand.grid(laplace = c(0, 1, 10, 50, 100, 200))
)
```

### 6. Naive Bayes (Gaussian)

-   laplace:

```{r}
nb_gaussian <- list(
  type = "Classification",
  library = "naivebayes",
  multiclass = TRUE,
  params = data.frame(),
  fit = nb_gaussian_fit,
  predict = predict.nb_gaussian_fit
)
```

### 7. Probability forest (grf)

-   mtry:Number of variables tried for each split\
    As none of my dataset has more than 11 covariates, this is set to ncol(X) (default) and not tuned

-   min.node.size:

-   alpha: controls maximum imbalance of split

-   imbalance penalty: controls how harshly imbalanced splits are penalized

```{r}
probability_forest_grf <- list(
    type = "Classification",
    library = "grf",
    multiclass = TRUE,
    params = data.frame(parameter = c("mtry", "min.node.size", "alpha", "imbalance.penalty"), 
                        class = rep("numeric",4), 
                        label = c("Number of variables tried for each split",
                                  "minimum number of observations in each tree leaf",
                                  "controls maximum imbalance of split",
                                  "controls how harshly imbalanced splits are penalized"
                                  )),
    fit = probability_forest_fit,
    predict = predict.probability_forest_fit,
    grid = expand.grid(min.node.size = seq(5, 20, 5),
                       alpha = seq(0,0.1,0.03),
                       imbalance.penalty = seq(0,1,0.5)
    )
    )
```

### 8. Probability forest (ranger)

-   mtry:

-   min.node.size:

-   alpha: controls maximum imbalance of split

```{r}
probability_forest_ranger <- list(
  type = "Classification",
  library = "ranger",
  multiclass = TRUE,
  params = data.frame(parameter = c(""), class = c(""), label = c("")),
  fit = ranger_fit,
  predict = predict.ranger_fit,
  grid = expand.grid(
    mtry = ncol(X), #floor(x * c(.05, .15, .25, .333, .4)),
    min.node.size = c(1, 3, 5, 10), 
    replace = c(TRUE, FALSE),                               
    sample.fraction = c(.5, .63, .8, 1),
    splitrule = c("gini", "extratrees"),
    importance = c('none', 'impurity', 'impurity_corrected', 'permutation')
)
)
```

### 9. MLPC

-   size:

```{r}
mlpc <- list(
  type = "Classification",
  library = "nnet",
  multiclass = TRUE,
  params = data.frame(parameter = c("size"), class = c("numeric"), label = c("number of units in the hidden layer")),
  fit = mlpc_fit,
  predict = predict.mlpc_fit,
  grid = expand.grid(size=seq(1,10,1))
)
```

### 10. xgboost

-   eta: learning rate
-   gamma: minimum loss reduction required for a split
-   nrounds: \# of boosting iterations
-   min_child_weight: minimum sum of instance weight (hessian) needed in a child

```{r}
xgboost <- list(
  type = "Classification",
  library = "xgboost",
  multiclass = TRUE,
  params = data.frame(parameter = c("eta", "gamma", "nrounds", "min_child_weight", "max_depth", "subsample"), 
                      class = rep("numeric", 6), 
                      label = c("learning rate", "minimun loss reduction required for split", "# of boosting iterations", "minimum sum of instance weight (hessian) needed in a child", "maximum depth of a tree", "subsample ratio of the training instance")),
  fit = xgboost_fit,
  predict = predict.xgboost_fit,
  grid = expand.grid(eta = c(0.001,0.01,0.1,0.2),
                     gamma = seq(0,1,0.2),
                     nround = seq(5, 40, 5),
                     min_child_weight = seq(0,2,0.5),
                     max_depth = seq(3,10,2),
                     subsample = c(0.5,0.7,1))
)
# Randomly select 1000 rows
random_sample_indices <- sample(nrow(xgboost$grid), 1000, replace = FALSE)
xgboost$grid <- xgboost$grid[random_sample_indices, ]

```

## Fine tuning classifier

To adapt in ml_wrapper: - fit: add ... in function arg - predict: just return fit

This function, **`finetuning_soft_classifier`**, is designed for fine-tuning a soft classifier using a specified method. Here's a concise explanation:

The function takes input data (**`x`** and **`y`**), a classification method (**`method`**), and an optional number of iterations (**`iter`**). It performs a grid search over the hyperparameter space defined in **`method$grid`** to find the best combination of parameters that minimizes the Brier Score, a metric for assessing the accuracy of probabilistic predictions.

For each grid point, the function splits the data into training and validation sets, fits the model using the specified method and parameters, and computes the Brier Score on the validation set. This process is repeated for a specified number of iterations, and the average Brier Score is recorded.

Finally, the function identifies the set of parameters that resulted in the lowest average Brier Score and returns both the matrix of performance scores (**`bs_mat`**) and the best parameters (**`best_params`**) as output.

```{r, message=FALSE}
best_logit <- finetuning_soft_classifier(x=X, y=W , method=multinomial_logit)

best_logit_nnet <- finetuning_soft_classifier(x=X , y=W , method=multinomial_logit_nnet)

best_knn <- finetuning_soft_classifier(x=X, y=W , method=knn)

#best_knn_radius <- finetuning_soft_classifier(x=X, y=W , method=knn_radius)

best_nb_bernulli <- finetuning_soft_classifier(x=X, y=W , method=nb_bernulli)

#best_nb_gaussian <- finetuning_soft_classifier(x=X, y=W , method=nb_gaussian)

best_probability_forest_grf <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_grf)

best_probability_forest_ranger <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_ranger)

best_mlpc <- finetuning_soft_classifier(x=X, y=W, method=mlpc)

best_xgboost <- finetuning_soft_classifier(x=X, y=W, method=xgboost)

```


```{r}
best_logit$best_params

best_logit_nnet$best_params 

best_knn$best_params

#best_knn_radius$best_params

best_nb_bernulli$best_params

#best_nb_gaussian$best_params

best_probability_forest_grf$best_params

best_probability_forest_ranger$best_params

best_mlpc$best_params

best_xgboost$best_params
```


```{r}
tuning_results <- list(best_knn, best_logit, best_logit_nnet, best_nb_bernulli, best_probability_forest_grf, best_probability_forest_ranger, best_mlpc)

# Save the data.table to a file using saveRDS
saveRDS(tuning_results, file = paste0("sim_results/tuning_results_", dataname, ".rds"))

# Load the data.table back
#loaded_nested_dt <- readRDS("tuning_results_imbens.rds")

```

# Binary only

## Create binary data

The binary dataset is created by just keeping sample for treatment `1`and `2`.

```{}

dataset <- dataset %>% filter(W %in% c(1,2))

```

## Define binary only methods

### 11. Adaboost

```{}
adaboost <- list(
  type = "Classification",
  library = "fastadaboost",
  multiclass = TRUE,
  params = data.frame(parameter = c("nIter"), class = c("numeric"), label = c("")),
  fit = adaboost_fit,
  predict = predict.adaboost_fit,
  grid = expand.grid()
)
```

### 12. BART

```{}
bart <- list(
  type = "Classification",
  library = "bartMachine",
  multiclass = TRUE,
  params = data.frame(parameter = c("num_trees", "alpha", "beta"), class = c("numeric"), label = c("")),
  fit = bart_fit,
  predict = predict.bart_fit,
  grid = expand.grid()
)
```

### 13. SVM

```{}
svm <- list(
  type = "Classification",
  library = "e1017",
  multiclass = TRUE,
  params = data.frame(parameter = c(""), class = c("numeric"), label = c("")),
  fit = svm_fit,
  predict = predict.svm_fit,
  grid = expand.grid()
)
```
