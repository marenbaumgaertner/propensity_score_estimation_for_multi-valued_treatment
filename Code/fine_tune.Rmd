---
title: "Fine tuning hyperparameter"
author: "Maren Baumg√§rtner"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

```{r, message=FALSE, echo=FALSE,warning=FALSE}
source("packages.R")
```

```{r}
source("ml_wrapper.R")
source("eval_functions.R")

set.seed(42)
options(scipen= 999)
```

Define inherently multi-valued classifieres

```{r}
classifiers <- c(
  "knn", "knn_radius", "lda", "logit",
  "mlpc", "multinom", "nb_bernulli", "nb_gaussian",
  "probability_forest", "qda", "ranger", "xgboost"
)

classifiers_to_tune <- classifiers[!(classifiers %in% c("lda", "qda"))]
```

# Create multi-valued train data

```{r}
n=2000
K=3
p=15

W <- sample(seq(1, K, by = 1), replace = TRUE, n) # random W
X = matrix(runif(n*p,-pi,pi),ncol=p)
#X = matrix(sample(-0:2, n * p, replace = TRUE), nrow = n, ncol = p)

# Define a function to assign probabilities based on the sign of X[1]
assign_probabilities <- function(x) {
  p_class_1 <- ifelse(x[1] < 0, 0.7, 0.15)
  p_class_3 <- ifelse(x[1] > 0, 0.7, 0.15)
  p_class_2 <- 1 - p_class_1 - p_class_3
  probabilities <- c(p_class_1, p_class_2, p_class_3)
  return(probabilities)
}

# Apply the function to each row of X to get probabilities for W
probabilities <- t(apply(X, 1, assign_probabilities))

# Randomly sample W based on the probabilities
W <- apply(probabilities, 1, function(p) sample(1:K, size = 1, prob = p))

W.levels <- sort(unique(W))
K <- length(W.levels)

dataset <- data.frame(W=W,X)

split_indices <- sample.split(dataset$W, SplitRatio = 0.8)
training_data <- dataset[split_indices, ]
test_data <- dataset[!split_indices, ]

W_training <- as.matrix(data.frame(W = training_data$W))
X_training <- as.matrix(data.frame(X[split_indices, ]))
W_test <-     as.matrix(data.frame(W = test_data$W))
X_test <-     as.matrix(data.frame(X[!split_indices, ]))
```

# Define methods

For each classifier I create a list which contains information on the classifier:

1.  Method Type:\
    Define for which type of problem the method is suitable. In my case it is all classification, right now this information is not used by the `finetuning_soft_classifier()` function, but as most of these methods are also able to do regession tasks it may be useful later on.

2.  Library Selection:

    Define the package the methods is using such that the correct package can be loaded by the `finetuning_soft_classifier()` function

3.  Parameters:

    The parameters argument contains a table of all used tuning parameters for the model. It contains information on the name of the input argument of the function, whether it is numeric or a character, and a definition of what the tuning parameter is doing.

4.  Fitting and Prediction Functions:

    Next, I define the corresponding `_fit()` and `predict.()` function.

5.  Tuning Grid:

    Lastly, I define the tuning grid for tuning parameter optimization. This grid allows the algorithm to find the optimal combination of parameters that results in the best model performance.

## 1. Multinomial logistic regression (glmnet)

-   alpha: Mixing Percentage

-   lambda: Regularization

```{r}
# define new method
multinomial_logit <- list(type = "Classification",
              library = "glmnet",
              multiclass = TRUE) 
# define parameters
multinomial_logit$parameters <- data.frame(parameter = c("alpha", "lambda"),
                  class = rep("numeric", 2),
                  label = c("Mixing Percentage", "Regularization"))

multinomial_logit$fit <- logit_fit
multinomial_logit$predict <- predict.logit_fit
# design the parameter tuning grid
multinomial_logit$grid <- expand.grid(alpha = seq(0,1,0.5),
                    lambda = seq(0,1,0.5))
```

## 2. Multinomial logistic regression (nnet)

multinom calls nnet() with zero hidden layers (size=0)

-   alpha: Mixing Percentage

-   lambda: Regularization

```{r}
# define new method
multinomial_logit_nnet <- list(type = "Classification",
              library = "nnet",
              multiclass = TRUE) 
# define parameters
multinomial_logit_nnet$parameters <- data.frame(parameter = c("alpha", "lambda"),
                  class = rep("numeric", 2),
                  label = c("Mixing Percentage", "Regularization"))

multinomial_logit_nnet$fit <- logit_fit
multinomial_logit_nnet$predict <- predict.logit_fit
# design the parameter tuning grid
multinomial_logit_nnet$grid <- expand.grid(alpha = seq(0,1,0.5),
                    lambda = seq(0,1,0.5))

```

## 3. kNN

-   kmax: number of neighbors considered

-   distance: parameter of Minkowski distance

```{r}
knn <- list(type = "Classification",
            library = "kknn",
            multiclass = TRUE)
params <- data.frame(parameter = c("kmax", "distance", "kernel"),
                     class = c("numeric", "numeric", ""),
                     label = c("number of neighbors considered", 
                               "parameter of Minkowski distance",
                               "Kernel to use"))
knn$params <- params
knn$fit <- knn_fit
knn$predict <- predict.knn_fit
knn$grid <- expand.grid(kmax = seq(5,30,10),
                        #distance = seq(1,5,1),
                        kernel = c("rectangular", "gaussian", "optimal"))
```

## 4. kNN (Radius)

```{r}
knn_radius <- list(type = "Classification",
            library = "kknn",
            multiclass = TRUE)
params <- data.frame(parameter = c("distance", "kernel"),
                     class = c("numeric", ""),
                     label = c("parameter of Minkowski distance",
                               "Kernel to use"))
knn_radius$params <- params
knn_radius$fit <- knn_fit
knn_radius$predict <- predict.knn_fit
knn_radius_grid <- expand.grid(distance = seq(1,5,1),
                        kernel = c("rectangular", "gaussian", "optimal"))
knn_radius$grid <- knn_radius_grid
```

## 5. Naive Bayes (Bernulli)

-   laplace:

```{r}
nb_bernulli <- list(
  type = "Classification",
  library = "naivebayes",
  multiclass = TRUE,
  params = data.frame(parameter = c("laplace"), 
                      class = c("numeric"), 
                      label = c("value of Laplace smoothing")),
  fit = nb_bernulli_fit,
  predict = predict.nb_bernulli_fit,
  grid = expand.grid(laplace = seq(0, 20, 2))
)
```

## 6. Naive Bayes (Gaussian)

-   laplace:

```{r}
nb_gaussian <- list(
  type = "Classification",
  library = "naivebayes",
  multiclass = TRUE,
  params = data.frame(parameter = c("laplace"), class = c("numeric"), label = c("value of Laplace smoothing")),
  fit = nb_gaussian_fit,
  predict = predict.nb_gaussian_fit,
  grid = expand.grid(laplace = seq(0, 20, 2))
)
```

## 7. Probability forest (grf)

-   mtry:

-   min.node.size:

-   alpha: controls maximum imbalance of split

-   imbalance penalty: controls how harshly imbalanced splits are penalized

```{r}
probability_forest_grf <- list(
    type = "Classification",
    library = "grf",
    multiclass = TRUE,
    params = data.frame(parameter = c("mtry", "min.node.size", "alpha"), class = c("numeric", "numeric", "numeric"), label = c("")),
    fit = probability_forest_fit,
    predict = predict.probability_forest_fit,
    grid = expand.grid(mtry = seq(as.integer(sqrt(ncol(X))), as.integer(sqrt(ncol(X)))+6,2),
    min.node.size = seq(5, 20, 5),
    alpha = seq(0.05,0.1,0.03)
    )
    )
```

## 8. Probability forest (ranger)

-   mtry:

-   min.node.size:

-   alpha: controls maximum imbalance of split

```{r}
probability_forest_ranger <- list(
  type = "Classification",
  library = "ranger",
  multiclass = TRUE,
  params = data.frame(parameter = c(""), class = c(""), label = c("")),
  fit = ranger_fit,
  predict = predict.ranger_fit,
  grid = expand.grid(mtry = seq(as.integer(sqrt(ncol(X))), as.integer(sqrt(ncol(X)))+6,2),
                     min.node.size = seq(5, 20, 5),
                     alpha = seq(0.05,0.1,0.03)
                     )
)
```

## 9. MLPC

-   size:

```{r}
mlpc <- list(
  type = "Classification",
  library = "nnet",
  multiclass = TRUE,
  params = data.frame(parameter = c("size"), class = c("numeric"), label = c("number of hidden layers")),
  fit = mlpc_fit,
  predict = predict.mlpc_fit,
  grid = expand.grid(size=seq(1,3,1))
)
```

## 10. xgboost

-   eta: learning rate
-   gamma: minimum loss reduction required for a split
-   nrounds: \# of boosting iterations
-   min_child_weight: minimum sum of instance weight (hessian) needed in a child

```{r}
xgboost <- list(
  type = "Classification",
  library = "xgboost",
  multiclass = TRUE,
  params = data.frame(parameter = c("eta", "gamma", "nrounds", "min_child_weight"), class = c("numeric", "numeric", "numeric", "numeric"), label = c("learning rate", "minimun loss reduction required for split", "# of boosting iterations", "minimum sum of instance weight (hessian) needed in a child")),
  fit = xgboost_fit,
  predict = predict.xgboost_fit,
  grid = expand.grid(eta = seq(0.1,0.5,0.1),
                     gamma = seq(0,1,0.2),
                     nround = seq(5, 100, 10),
                     min_child_weight = seq(0,2,0.5))
)
```

# Fine tuning classifier

To adapt in ml_wrapper: - fit: add ... in function arg - predict: just return fit

This function, **`finetuning_soft_classifier`**, is designed for fine-tuning a soft classifier using a specified method. Here's a concise explanation:

The function takes input data (**`x`** and **`y`**), a classification method (**`method`**), and an optional number of iterations (**`iter`**). It performs a grid search over the hyperparameter space defined in **`method$grid`** to find the best combination of parameters that minimizes the Brier Score, a metric for assessing the accuracy of probabilistic predictions.

For each grid point, the function splits the data into training and validation sets, fits the model using the specified method and parameters, and computes the Brier Score on the validation set. This process is repeated for a specified number of iterations, and the average Brier Score is recorded.

Finally, the function identifies the set of parameters that resulted in the lowest average Brier Score and returns both the matrix of performance scores (**`bs_mat`**) and the best parameters (**`best_params`**) as output.

```{r, message=FALSE}
best_logit <- finetuning_soft_classifier(x=X_training, y=W_training, method=multinomial_logit)

best_logit_nnet <- finetuning_soft_classifier(x=X_training, y=W_training, method=multinomial_logit_nnet)

best_knn <- finetuning_soft_classifier(x=X_training, y=W_training, method=knn)

best_knn_radius <- finetuning_soft_classifier(x=X_training, y=W_training, method=knn_radius)

best_nb_bernulli <- finetuning_soft_classifier(x=X_training, y=W_training, method=nb_bernulli)

best_nb_gaussian <- finetuning_soft_classifier(x=X_training, y=W_training, method=nb_gaussian)

best_probability_forest_grf <- finetuning_soft_classifier(x=X_training,y=W_training, method=probability_forest_grf)

best_probability_forest_ranger <- finetuning_soft_classifier(x=X_training, y=W_training, method=probability_forest_ranger)

best_mlpc <- finetuning_soft_classifier(x=X_training, y=W_training, method=mlpc)

best_xgboost <- finetuning_soft_classifier(x=X_training, y=W_training, method=xgboost)
```

```{r}
best_mlpc$best_params
```
