---
title: "Fine tuning hyperparameter"
author: "Maren Baumg√§rtner"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

```{r, message=FALSE, echo=FALSE,warning=FALSE}
source("functions/packages.R")
```

```{r}
source("functions/ml_wrapper.R")
source("functions/eval_functions.R")

#set.seed(42)
options(scipen= 999)
```

This notebook contain the fine tuning procedure for the multi-class classifiers for all three datasets. The set of classification models contains ten different multi-class algorithms namely kNN, LDA, MLPC, multinomial logistic regression (\texttt{glmnet} vs. \texttt{nnet}), naive Bayes (Bernoulli vs. Gaussian), probability forest (\texttt{grf} vs. \texttt{ranger}) and QDA. 

# Multi-valued

First I define the multi-class classifieres. For the classifiers LDA and QDA there are no meaningful tuning parameters as well as for the Gaussian naive Bayes classifier.

```{r}
classifiers <- c(
  "knn", "lda", "logit",
  "mlpc", "logit_nnet", "nb_bernoulli", "nb_gaussian",
  "probability_forest", "qda", "ranger"
)

classifiers_to_tune <- classifiers[!(classifiers %in% c("lda", "qda", "nb_gaussian"))]
```


## Define methods

For each classifier I create a list which contains information on the classifier:

1.  Method Type:\
    Define for which type of problem the method is suitable. In my case it is all classification, right now this information is not used by the `finetuning_soft_classifier()` function, but as most of these methods are also able to do regession tasks it may be useful later on.

2.  Library Selection:

    Define the package the methods is using such that the correct package can be loaded by the `finetuning_soft_classifier()` function

3.  Parameters:

    The parameters argument contains a table of all used tuning parameters for the model. It contains information on the name of the input argument of the function, whether it is numeric or a character, and a definition of what the tuning parameter is doing.

4.  Fitting and Prediction Functions:

    Next, I define the corresponding `_fit()` and `predict.()` function.

5.  Tuning Grid:

    Lastly, I define the tuning grid for tuning parameter optimization. This grid allows the algorithm to find the optimal combination of parameters that results in the best model performance.

### 1. Multinomial logistic regression (glmnet)

-   alpha: Mixing Percentage

-   lambda: Regularization

```{}
multinomial_logit <- list(type = "Classification",
                          library = "glmnet",
                          multiclass = TRUE) 
# define parameters
multinomial_logit$parameters <- data.frame(parameter = c("alpha", "lambda"),
                                           class = rep("numeric", 2),
                                           label = c("Mixing Percentage", "Regularization"))

multinomial_logit$fit <- logit_fit
multinomial_logit$predict <- predict.logit_fit
# design the parameter tuning grid
multinomial_logit$grid <- expand.grid(alpha = seq(0,1,0.05))#,
```

### 2. Multinomial logistic regression (nnet)

multinom calls nnet() with zero hidden layers (size=0)

-   alpha: Mixing Percentage

-   lambda: Regularization

```{}
multinomial_logit_nnet <- list(type = "Classification",
                               library = "nnet",
                               multiclass = TRUE) 
# define parameters
multinomial_logit_nnet$parameters <- data.frame(parameter = c("decay"),
                                                class = c("numeric"),
                                                label = c("weight decay"))

multinomial_logit_nnet$fit <- logit_nnet_fit
multinomial_logit_nnet$predict <- predict.logit_nnet_fit
# design the parameter tuning grid
multinomial_logit_nnet$grid <- expand.grid(decay = seq(0,1,0.05))
```

### 3. kNN

-   kmax: number of neighbors considered

-   distance: parameter of Minkowski distance

kknn() function tunes itself 

```{}
knn <- list(type = "Classification",
            library = "kknn",
            multiclass = TRUE)
params <- data.frame(parameter = c("kmax", "distance", "kernel"),
                     class = c("numeric", "numeric", ""),
                     label = c("number of neighbors considered", 
                               "parameter of Minkowski distance",
                               "Kernel to use"))
knn$params <- params
knn$fit <- knn_fit
knn$predict <- predict.knn_fit
knn$grid <- expand.grid(kmax = seq(5,30,5),
                        kernel = c("rectangular", "gaussian", "optimal"))
```


### 4. Naive Bayes (Bernoulli)

-   laplace:

```{}
nb_bernoulli <- list(
  type = "Classification",
  library = "naivebayes",
  multiclass = TRUE,
  params = data.frame(parameter = c("laplace"), 
                      class = c("numeric"), 
                      label = c("value of Laplace smoothing")),
  fit = nb_bernulli_fit,
  predict = predict.nb_bernulli_fit,
  grid = expand.grid(laplace = c(0, 1, 10, 50, 100, 200))
)
```

### 5. Probability forest (grf)

-   mtry:Number of variables tried for each split\
    As none of my dataset has more than 11 covariates, this is set to ncol(X) (default) and not tuned

-   min.node.size:

-   alpha: controls maximum imbalance of split

-   imbalance penalty: controls how harshly imbalanced splits are penalized

```{}
probability_forest_grf <- list(
  type = "Classification",
  library = "grf",
  multiclass = TRUE,
  params = data.frame(parameter = c("mtry", "min.node.size", "alpha", "imbalance.penalty", "num.trees"), 
                      class = rep("numeric",5), 
                      label = c("Number of variables tried for each split",
                                "minimum number of observations in each tree leaf",
                                "controls maximum imbalance of split",
                                "controls how harshly imbalanced splits are penalized",
                                "number of trees grown"
                      )),
  fit = probability_forest_fit,
  predict = predict.probability_forest_fit,
  grid = expand.grid(min.node.size = seq(5, 20, 5),
                     alpha = seq(0,0.1,0.03),
                     imbalance.penalty = seq(0,1,0.5),
                     num.trees=1000
  )
)
```

### 6. Probability forest (ranger)

-   mtry:

-   min.node.size:

-   alpha: controls maximum imbalance of split

```{}
probability_forest_ranger <- list(
  type = "Classification",
  library = "ranger",
  multiclass = TRUE,
  params = data.frame(parameter = c(""), class = c(""), label = c("")),
  fit = ranger_fit,
  predict = predict.ranger_fit,
  grid = expand.grid(
    mtry = ncol(X), #floor(x * c(.05, .15, .25, .333, .4)),
    min.node.size = seq(10, 20,5), 
    replace = c(TRUE, FALSE),                               
    sample.fraction = c(.5, .66, .8, 1),
    splitrule = c("gini", "extratrees"),
    importance = c('none', 'impurity', 'permutation'),
    num.trees = 1000
  )
)
```

### 7. MLPC

-   size:

```{}
mlpc <- list(
  type = "Classification",
  library = "nnet",
  multiclass = TRUE,
  params = data.frame(parameter = c("size"), class = c("numeric"), label = c("number of units in the hidden layer")),
  fit = mlpc_fit,
  predict = predict.mlpc_fit,
  grid = expand.grid(size=seq(1,10,1))
)
```


## Fine tuning classifier

This function, **`finetuning_soft_classifier`**, is designed for fine-tuning a soft classifier using a specified method. Here's a concise explanation:

The function takes input data (**`x`** and **`y`**), a classification method (**`method`**), and an optional number of iterations (**`iter`**). It performs a grid search over the hyperparameter space defined in **`method$grid`** to find the best combination of parameters that minimizes the Brier Score, a metric for assessing the accuracy of probabilistic predictions.

For each grid point, the function splits the data into training and validation sets, fits the model using the specified method and parameters, and computes the Brier Score on the validation set. This process is repeated for a specified number of iterations, and the average Brier Score is recorded.

Finally, the function identifies the set of parameters that resulted in the lowest average Brier Score and returns both the matrix of performance scores (**`bs_mat`**) and the best parameters (**`best_params`**) as output.

## All datasets

The hyperparameters are tuned prior to conducting the simulation study rather than adjusting them individually for each iteration. However, tuning them only once might lack precision. Therefore, I generate 10 datasets from a given DGP and fine-tune the classifiers on each dataset, resulting in 10 optimal values for each tuning parameter of each classifier. The optimal values for the hyperparameters are then determined by using either the median or the mode for non-numeric parameters.

```{r}

# define vector the names of the datasets
datanames <- c("Imbens_2016","Linden_2015", "Acharky_2023")

# outer loop over datasets
for (dataname in datanames){
  datapath <- paste0("data/data_", dataname,".R")
  
  n_iter <- 10  # Example value, replace with your desired number of iterations
  
  # Create an empty list to store results
  results <- list()
  
  
  start_time <- Sys.time()  # Record start time
  
  for (iter in 1:n_iter) {
    set.seed(iter)
    # Initialize sublist for current iteration
    iter_results <- list()
    
    # generate new data per iterations
    source(datapath)
    print(head(X))
    
    # source ml_methods such that parameters dependent on ncol(X) are defined correctly
    source("functions/ml_methods.R")
  
    
    best_logit <- finetuning_soft_classifier(x=X, y=W, method=multinomial_logit)
    iter_results$multinomial_logit <- best_logit
    
    best_logit_nnet <- finetuning_soft_classifier(x=X, y=W, method=multinomial_logit_nnet)
    iter_results$multinomial_logit_nnet <- best_logit_nnet
    
    best_knn <- finetuning_soft_classifier(x=X, y=W, method=knn)
    iter_results$knn <- best_knn
    
    best_nb_bernoulli <- finetuning_soft_classifier(x=X, y=W, method=nb_bernoulli)
    iter_results$nb_bernoulli <- best_nb_bernoulli
    
    best_probability_forest_grf <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_grf)
    iter_results$probability_forest_grf <- best_probability_forest_grf
    
    best_probability_forest_ranger <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_ranger)
    iter_results$probability_forest_ranger <- best_probability_forest_ranger
    
    best_mlpc <- finetuning_soft_classifier(x=X, y=W, method=mlpc)
    iter_results$mlpc <- best_mlpc
    
    # Append results of current iteration to main results list
    results[[iter]] <- iter_results
  }
  
  end_time <- Sys.time()  # Record end time
  elapsed_time <- end_time - start_time
  cat(n_iter, "Iterations:", "elapsed time:", elapsed_time, "\n")
  
  saveRDS(results, file = paste0("sim_results/tuning_results_", dataname, "_", n_iter, "_iterations.rds"))

}
```



## Single dataset
This chunk contains only the tuning procedure for one dataset, respectively anything but the outer loop over the datasets such that one can tune a dataset separately.
```{r, eval=FALSE}

#dataname <- "Imbens_2016"
#dataname <- "Linden_2015"
dataname <- "Archarky_2023"

datapath <- paste0("data/data_", dataname,".R")
n_iter <- 10  # Example value, replace with your desired number of iterations

# Create an empty list to store results
results <- list()


start_time <- Sys.time()  # Record start time

for (iter in 1:n_iter) {
  set.seed(iter)

  # Initialize sublist for current iteration
  iter_results <- list()
  
  # generate new data per iterations
  source(datapath)
  print(head(X))
  
  source("ml_methods.R")
  
  best_logit <- finetuning_soft_classifier(x=X, y=W, method=multinomial_logit)
  iter_results$multinomial_logit <- best_logit
  
  best_logit_nnet <- finetuning_soft_classifier(x=X, y=W, method=multinomial_logit_nnet)
  iter_results$multinomial_logit_nnet <- best_logit_nnet
  
  best_knn <- finetuning_soft_classifier(x=X, y=W, method=knn)
  iter_results$knn <- best_knn
  
  best_nb_bernoulli <- finetuning_soft_classifier(x=X, y=W, method=nb_bernoulli)
  iter_results$nb_bernoulli <- best_nb_bernoulli
  
  best_probability_forest_grf <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_grf)
  iter_results$probability_forest_grf <- best_probability_forest_grf
  
  best_probability_forest_ranger <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_ranger)
  iter_results$probability_forest_ranger <- best_probability_forest_ranger
  
  best_mlpc <- finetuning_soft_classifier(x=X, y=W, method=mlpc)
  iter_results$mlpc <- best_mlpc
  
  best_xgboost <- finetuning_soft_classifier(x=X, y=W, method=xgboost_method)
  iter_results$xgboost_method <- best_xgboost
  
  # Append results of current iteration to main results list
  results[[iter]] <- iter_results
}

end_time <- Sys.time()  # Record end time
elapsed_time <- end_time - start_time
cat(n_iter, "Iterations:", "elapsed time:", elapsed_time, "\n")

saveRDS(results, file = paste0("sim_results/tuning_results_", dataname, "_", n_iter, "_iterations.rds"))


```






```{r, echo=FALSE, eval=FALSE}
rm(list = ls())
gc()
## Define binary only methods

```



```{r, echo=FALSE, eval=FALSE}
### 11. Adaboost

adaboost <- list(
  type = "Classification",
  library = "fastadaboost",
  multiclass = TRUE,
  params = data.frame(parameter = c("nIter"), class = c("numeric"), label = c("")),
  fit = adaboost_fit,
  predict = predict.adaboost_fit,
  grid = expand.grid()
)
```


```{r, echo=FALSE, eval=FALSE}
### 12. BART

bart <- list(
  type = "Classification",
  library = "bartMachine",
  multiclass = TRUE,
  params = data.frame(parameter = c("num_trees", "alpha", "beta"), class = c("numeric"), label = c("")),
  fit = bart_fit,
  predict = predict.bart_fit,
  grid = expand.grid()
)
```


```{r, echo=FALSE, eval=FALSE}
### 13. SVM

svm <- list(
  type = "Classification",
  library = "e1017",
  multiclass = TRUE,
  params = data.frame(parameter = c("gamma"), class = c("numeric"), label = c("")),
  fit = svm_fit,
  predict = predict.svm_fit,
  grid = expand.grid()
)
```

```{r, echo=FALSE, eval=FALSE}
## All datasets (binary)

datanames <- c("Imbens_2016","Linden_2015", "Archarky_2023")

for (dataname in datanames){
  datapath <- paste0("data/data_", dataname,".R")
  
  n_iter <- 10  # Example value, replace with your desired number of iterations
  
  # Create an empty list to store results
  results <- list()
  
  
  start_time <- Sys.time()  # Record start time
  
  for (iter in 1:n_iter) {
    set.seed(iter)
    # Initialize sublist for current iteration
    iter_results <- list()
    
    # generate new data per iterations
    source(datapath)
    print(head(X))
    
    
    
    source("ml_methods.R")
  
    
    best_logit <- finetuning_soft_classifier(x=X, y=W, method=multinomial_logit)
    iter_results$multinomial_logit <- best_logit
    
    best_logit_nnet <- finetuning_soft_classifier(x=X, y=W, method=multinomial_logit_nnet)
    iter_results$multinomial_logit_nnet <- best_logit_nnet
    
    best_knn <- finetuning_soft_classifier(x=X, y=W, method=knn)
    iter_results$knn <- best_knn
    
    best_nb_bernoulli <- finetuning_soft_classifier(x=X, y=W, method=nb_bernoulli)
    iter_results$nb_bernoulli <- best_nb_bernoulli
    
    best_probability_forest_grf <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_grf)
    iter_results$probability_forest_grf <- best_probability_forest_grf
    
    best_probability_forest_ranger <- finetuning_soft_classifier(x=X, y=W, method=probability_forest_ranger)
    iter_results$probability_forest_ranger <- best_probability_forest_ranger
    
    best_mlpc <- finetuning_soft_classifier(x=X, y=W, method=mlpc)
    iter_results$mlpc <- best_mlpc
    
    best_xgboost <- finetuning_soft_classifier(x=X, y=W, method=xgboost_method)
    iter_results$xgboost_method <- best_xgboost
    
    
    
    # Append results of current iteration to main results list
    results[[iter]] <- iter_results
  }
  
  end_time <- Sys.time()  # Record end time
  elapsed_time <- end_time - start_time
  cat(n_iter, "Iterations:", "elapsed time:", elapsed_time, "\n")
  
  saveRDS(results, file = paste0("sim_results/tuning_results_", dataname, "_", n_iter, "_iterations.rds"))

}
```
